{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "We want to check whether our identification of face artifacts from street network polygons is correct. We postulate that, in accordance with our definitions:\n",
    "\n",
    "1. **no** face artifacts contain buildings (with a possible small margin of error, e.g. there could be a bus stop tagged as \"building\" placed in the middle of a multilane street);\n",
    "2. **some** urban blocks contain **no** buildings (e.g. squares, parks, sports and service areas).\n",
    "For an automated validation of our findings, we can use building data from OSM to check statement 1:\n",
    "Polygons classified as face artifacts, but which contain buildings, are wrongly classified and actually represent urban blocks (type 1 error). Note that an automated checking of statement 2 - i.e. actual face artifacts wrongly classified as urban blocks - is not possible since we have no ground truth (e.g. a list of \"empty urban blocks\") to test agains.\n",
    "\n",
    "Thus, in this notebook we:\n",
    "* download building data for all FUAs from OSM\n",
    "* visually check FUA building data for sufficient completeness\n",
    "* for the subset of FUAs with a sufficient completeness of building data on OSM,\n",
    "    - import street network face polygons\n",
    "    - classify them into face artifacts vs. urban blocks according to the face artifact threshold found for the FUA in Notebook 4 (peaks)\n",
    "    - for each polygon, compute its area of intersection with the building data set\n",
    "    - generate pseudo-confusion matrices for different tolerance thresholds (the threshold indicates how many square meters of building surface a face artifact is allowed contain without being classified as urban block)\n",
    "    - visualize results of validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shapely 2.0.0.!\n",
    "import shapely\n",
    "from shapely import strtree\n",
    "from shapely.geometry import LineString\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "# import pygeos\n",
    "\n",
    "import geopandas\n",
    "import pandas\n",
    "import dask_geopandas\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from palettable.cartocolors.qualitative import Bold_6\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import osmnx as ox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import meta data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample meta data\n",
    "sample = geopandas.read_parquet(\"../data/sample.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download building data for each FUA from OSM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for the last 3 cities (london, cologne, dortmund) - to run on HPC\n",
    "sample.drop(sample[sample[\"eFUA_name\"].isin([\"London\", \"Cologne\", \"Dortmund\"])].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter warnings about GeoParquet implementation.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*initial implementation of Parquet.*\")\n",
    "\n",
    "for ix, row in tqdm(sample.iterrows(), total=len(sample)):\n",
    "    city = row.eFUA_name\n",
    "\n",
    "    # check if file exists - if it doesn't, download from OSM:\n",
    "    if os.path.exists(f\"../data/buildings/buildings_{city}.gpkg\"):\n",
    "        print(f\"{city} building data already downloaded\")\n",
    "    elif os.path.exists(f\"../data/buildings/buildings_{city}_clean.gpkg\"):\n",
    "        print(f\"{city} building data already downloaded and preprocessed\")\n",
    "    else:\n",
    "        print(f\"dowloading {city} building data\")\n",
    "        # Download OSM buildings\n",
    "        buildings = ox.geometries_from_polygon(\n",
    "            sample[sample[\"eFUA_name\"] == city].geometry.values[0],\n",
    "            tags={\"building\": True},\n",
    "        )\n",
    "        buildings_small = buildings[[\"geometry\"]]  # saving only most relevant columns\n",
    "        buildings_small.to_file(f\"../data/buildings/buildings_{city}.gpkg\", index=False)\n",
    "        del (buildings, buildings_small)\n",
    "        print(f\"{city} building data downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual processing step in QGIS\n",
    "\n",
    "In this step, for each FUA, we visually checked in QGIS (by plotting and comparing all buildings, the OSM base layer, and the FUA shape) whether its building mapping in OSM is sufficiently complete as to be used for face artifact validation purposes. Next, in the list for each continent, we noted down cities with enough buildings mapped for validation (in parenthesis: cities where building data from OSM would be sufficient for validation, but no face artifact index treshold was found).\n",
    "\n",
    "**Africa:** Abidjan, Conakry, Douala, Modagishu\n",
    "\n",
    "**Asia:** Aleppo, Kabul, Semarang, Seoul\n",
    "\n",
    "**South America:** Bucaramanga, Porto Alegre\n",
    "\n",
    "**North America:** Cincinnati, Dallas, Ottawa, Raleigh, Richmond, Rio Piedras [San Juan], Salt Lake City, San José, Washington D.C.\n",
    "\n",
    "**Europe:** Amsterdam, Belgrade, Chelyabinsk, Helsinki, Katowice, Krakow, Liège, Nuremberg, Saratov, Vienna, Warsaw\n",
    "\n",
    "**Oceania:** Auckland\n",
    "\n",
    "**Discarded cities**, i.e. cities with enough buildings mapped for validation, but no artifact index threshold found: Comilla, Dhaka, Jombang, Monrovia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define FUA subsample for validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_for_validation = [\n",
    "    \"Abidjan\",\n",
    "    \"Conakry\",\n",
    "    \"Douala\",\n",
    "    \"Mogadishu\",  # Africa\n",
    "    \"Aleppo\",\n",
    "    \"Kabul\",\n",
    "    \"Semarang\",\n",
    "    \"Seoul\",  # Asia\n",
    "    \"Bucaramanga\",\n",
    "    \"Porto Alegre\",  # South America\n",
    "    \"Cincinnati\",\n",
    "    \"Dallas\",\n",
    "    \"Ottawa\",\n",
    "    \"Raleigh\",\n",
    "    \"Richmond\",\n",
    "    \"Río Piedras [San Juan]\",\n",
    "    \"Salt Lake City\",\n",
    "    \"San Jose\",\n",
    "    \"Washington D.C.\",  # North America\n",
    "    \"Auckland\",  # Oceania\n",
    "    \"Amsterdam\",\n",
    "    \"Belgrade\",\n",
    "    \"Chelyabinsk\",\n",
    "    \"Helsinki\",\n",
    "    \"Katowice\",\n",
    "    \"Krakow\",\n",
    "    \"Liège\",\n",
    "    \"Nuremberg\",\n",
    "    \"Saratov\",\n",
    "    \"Vienna\",\n",
    "    \"Warsaw\",  # Europe\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing of building data for validation subsample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_validation = sample[sample.eFUA_name.isin(cities_for_validation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter warnings about GeoParquet implementation.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*initial implementation of Parquet.*\")\n",
    "\n",
    "for ix, row in tqdm(sample_validation.iterrows(), total=len(sample_validation)):\n",
    "    city = row.eFUA_name\n",
    "    myid = row.eFUA_ID\n",
    "\n",
    "    if os.path.exists(f\"../data/buildings/buildings_{city}_clean.gpkg\"):\n",
    "        print(f\"{city} building data already preprocessed\")\n",
    "    elif not os.path.exists(f\"../data/buildings/buildings_{city}.gpkg\"):\n",
    "        print(f\"{city} building data not downloaded yet\")\n",
    "    else:\n",
    "        polygons = geopandas.read_parquet(f\"../data/{int(row.eFUA_ID)}/polygons/\")\n",
    "        buildings = geopandas.read_file(f\"../data/buildings/buildings_{city}.gpkg\")\n",
    "        buildings = buildings.to_crs(polygons.crs)\n",
    "\n",
    "        # drop tags (not needed for analysis)\n",
    "        buildings = buildings[[\"geometry\"]]\n",
    "\n",
    "        # drop points, assert we only have polygons in the gdf\n",
    "        buildings = buildings.drop(\n",
    "            buildings[buildings.geometry.type == \"Point\"].index, axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        buildings = buildings.drop(\n",
    "            buildings[buildings.geometry.type == \"LineString\"].index, axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # explode multipolygons\n",
    "        buildings = buildings.explode(index_parts=False)\n",
    "\n",
    "        # check that we now only have polygons in the data set\n",
    "        assert all(buildings.geometry.type == \"Polygon\")\n",
    "\n",
    "        # save \"cleaned\" data set\n",
    "        buildings.to_file(f\"../data/buildings/buildings_{city}_clean.gpkg\", index=0)\n",
    "\n",
    "        # delete original data set\n",
    "        os.remove(f\"../data/buildings/buildings_{city}.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in peak results (face artifact thresholds)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = json.load(open(\"../results/04_peaks_results.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify urban blocks and calculate block/building overlap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/validation_temp\", exist_ok=True)\n",
    "\n",
    "# Filter warnings about GeoParquet implementation.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*initial implementation of Parquet.*\")\n",
    "\n",
    "for ix, row in tqdm(sample_validation.iterrows(), total=len(sample_validation)):\n",
    "    city = row.eFUA_name\n",
    "    myid = int(row.eFUA_ID)\n",
    "\n",
    "    # read in buildings and polygons\n",
    "    buildings = geopandas.read_file(f\"../data/buildings/buildings_{city}_clean.gpkg\")\n",
    "    polygons = geopandas.read_parquet(f\"../data/{myid}/polygons/\")\n",
    "    assert buildings.crs == polygons.crs\n",
    "\n",
    "    # find predicted artifacts\n",
    "    option = \"circular_compactness_index\"\n",
    "    threshold = results[city][option][\"threshold\"]\n",
    "    polygons[\"is_artifact\"] = False # set default to False\n",
    "    polygons.loc[\n",
    "        polygons[\"circular_compactness_index\"] <= threshold, \"is_artifact\"\n",
    "    ] = True # set to True for polygons where index is below the threshold\n",
    "\n",
    "    # find overlap of polygons with buildings\n",
    "    mytree = strtree.STRtree(geoms=buildings.geometry)\n",
    "    q = mytree.query(polygons.geometry, predicate=\"intersects\")\n",
    "    # q[0] ...polygon indeces\n",
    "    # q[1] ...building indeces\n",
    "\n",
    "    # find out whether polygons contain buldings or not\n",
    "    polygons[\"intersects_buildings\"] = False  # set default to False\n",
    "    polygons.loc[numpy.unique(q[0]), \"intersects_buildings\"] = True\n",
    "    # if index is in query, change to True\n",
    "\n",
    "    #### find absolute value of built area in polygon [in square meters]\n",
    "\n",
    "    # column of total area of polygon\n",
    "    polygons[\"area\"] = polygons.area\n",
    "\n",
    "    # column of building area within polygon (default to 0)\n",
    "    polygons[\"intersection_area\"] = 0\n",
    "\n",
    "    # replace 0 with area of unary union of buildings intersecting that polygon\n",
    "    for polygon_index in numpy.unique(q[0]):\n",
    "        # for this polygon, get indeces of buildings intersecting it\n",
    "        building_indeces = q[1][numpy.where(q[0] == polygon_index)]\n",
    "\n",
    "        # get area of intersection\n",
    "        intersection_area = (\n",
    "            polygons.loc[polygon_index, \"geometry\"]\n",
    "            .intersection(buildings.loc[building_indeces, \"geometry\"].unary_union)\n",
    "            .area\n",
    "        )\n",
    "\n",
    "        # add to polygon table\n",
    "        polygons.loc[polygon_index, \"intersection_area\"] = intersection_area\n",
    "\n",
    "    # drop not needed columns that were created in the process\n",
    "    polygons = polygons.drop(columns = [\"area\", \"intersects_buildings\"])\n",
    "\n",
    "    # save polygons to a partitioned GeoParquet, overwriting original polygons\n",
    "    polygons = dask_geopandas.from_geopandas(polygons, npartitions=10)\n",
    "    polygons.to_parquet(f\"../data/{int(myid)}/polygons/\")\n",
    "\n",
    "    del(buildings, polygons, mytree, q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PSEUDE CONFUSION MATRICES ###\n",
    "# for an area threshold of X:\n",
    "# true positive: is_artifact==True and intersection_area < X\n",
    "# true negative: is_artifact==False and intersection_area => X\n",
    "# false positive: is_artifact==True and intersection_area => X\n",
    "# false negative: is_artifact==False and intersection_area < X\n",
    "# note:\n",
    "#   true/false positives make sense (bananas != buildings) BUT\n",
    "#   true/false negatives make less sense (blocks == buildings is not ALWAYS the case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "35f35e1f0bb92e7ac05765c87ada263d1a2173e12d4a4460e46e5d1567b982a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
