{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "We want to check whether our identification of face artifacts from street network polygons is correct. We postulate that, in accordance with our definitions:\n",
    "\n",
    "1. **no** face artifacts contain buildings (with a possible small margin of error, e.g. there could be a bus stop tagged as \"building\" placed in the middle of a multilane street);\n",
    "2. **some** urban blocks contain **no** buildings (e.g. squares, parks, sports and service areas).\n",
    "For an automated validation of our findings, we can use building data from OSM to check statement 1:\n",
    "Polygons classified as face artifacts, but which contain buildings, are wrongly classified and actually represent urban blocks (type 1 error). Note that an automated checking of statement 2 - i.e. actual face artifacts wrongly classified as urban blocks - is not possible since we have no ground truth (e.g. a list of \"empty urban blocks\") to test agains.\n",
    "\n",
    "Thus, in this notebook we:\n",
    "* download building data for all FUAs from OSM\n",
    "* visually check FUA building data for sufficient completeness\n",
    "* for the subset of FUAs with a sufficient completeness of building data on OSM,\n",
    "    - import street network face polygons\n",
    "    - classify them into face artifacts vs. urban blocks according to the face artifact threshold found for the FUA in Notebook 4 (peaks)\n",
    "    - for each polygon, compute its area of intersection with the building data set\n",
    "    - generate pseudo-confusion matrices for different tolerance thresholds (the threshold indicates how many square meters of building surface a face artifact is allowed contain without being classified as urban block)\n",
    "    - visualize results of validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shapely 2.0.0.!\n",
    "import shapely\n",
    "from shapely import strtree\n",
    "from shapely.geometry import LineString\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "\n",
    "import geopandas\n",
    "import pandas\n",
    "import dask_geopandas\n",
    "\n",
    "import numpy\n",
    "from palettable.cartocolors.qualitative import Bold_6\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import osmnx as ox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import meta data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample meta data\n",
    "sample = geopandas.read_parquet(\"../data/sample.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download building data for each FUA from OSM**\n",
    "\n",
    "We do this in 2 steps: \n",
    "\n",
    "1. first, for all cities except London, Cologne, and Dortmund - where building data for the entire FUA area can be downloaded in one go;\n",
    "2. and second, for London, Cologne, and Dortmund, where building data is too large to be downloaded in one go - we need to split the request polygon into smaller polygons, and then recombine the bulding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: For all cities where data can be downloaded in one go\n",
    "sample.drop(sample[sample[\"eFUA_name\"].isin([\"London\", \"Cologne\", \"Dortmund\"])].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter warnings about GeoParquet implementation.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*initial implementation of Parquet.*\")\n",
    "\n",
    "for ix, row in tqdm(sample.iterrows(), total=len(sample)):\n",
    "    city = row.eFUA_name\n",
    "\n",
    "    # check if file exists - if it doesn't, download from OSM:\n",
    "    if os.path.exists(f\"../data/buildings/buildings_{city}.gpkg\"):\n",
    "        print(f\"{city} building data already downloaded\")\n",
    "    elif os.path.exists(f\"../data/buildings/buildings_{city}_clean.gpkg\"):\n",
    "        print(f\"{city} building data already downloaded and preprocessed\")\n",
    "    else:\n",
    "        print(f\"dowloading {city} building data\")\n",
    "        # Download OSM buildings\n",
    "        buildings = ox.geometries_from_polygon(\n",
    "            sample[sample[\"eFUA_name\"] == city].geometry.values[0],\n",
    "            tags={\"building\": True},\n",
    "        )\n",
    "        buildings_small = buildings[[\"geometry\"]]  # saving only most relevant columns\n",
    "        buildings_small.to_file(f\"../data/buildings/buildings_{city}.gpkg\", index=False)\n",
    "        del (buildings, buildings_small)\n",
    "        print(f\"{city} building data downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: for London, Cologne, and Dortmund\n",
    "\n",
    "# sample meta data\n",
    "sample = geopandas.read_parquet(\"../data/sample.parquet\")\n",
    "\n",
    "# make a separate df of cities whose building data sets are too big to download in one go\n",
    "sample_big = sample[sample[\"eFUA_name\"].isin([\"London\", \"Dortmund\", \"Cologne\"])]\n",
    "\n",
    "# define a function to separate the FUA area into grid cells (to download building data separately)\n",
    "def create_grid_geometry(gdf, cell_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a geodataframe with grid cells covering the area specificed by the input gdf\n",
    "\n",
    "    Arguments:\n",
    "        gdf (gdf): geodataframe with a polygon/polygons defining the study area\n",
    "        cell_size (numeric): width of the grid cells in units used by gdf crs\n",
    "\n",
    "    Returns:\n",
    "        grid (gdf): gdf with grid cells in same crs as input data\n",
    "    \"\"\"\n",
    "\n",
    "    geometry = gdf[\"geometry\"].unary_union\n",
    "    geometry_cut = osmnx.utils_geo._quadrat_cut_geometry(geometry, quadrat_width=cell_size)\n",
    "\n",
    "    grid = geopandas.GeoDataFrame(geometry=[geometry_cut], crs=gdf.crs)\n",
    "\n",
    "    grid = grid.explode(index_parts=False, ignore_index=True)\n",
    "\n",
    "    # Create arbitraty grid id col\n",
    "    grid[\"grid_id\"] = grid.index\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the given index in the sample data frame,\n",
    "for ix in sample_big.index:\n",
    "\n",
    "    # get meta data for this city\n",
    "    eFUA_ID = int(sample_big.loc[ix, \"eFUA_ID\"])\n",
    "    eFUA_name = str(sample_big.loc[ix, \"eFUA_name\"])\n",
    "\n",
    "    # check if file exists - if it doesn't, download from OSM:\n",
    "    if os.path.exists(f\"../data/buildings/buildings_{eFUA_name}.gpkg\"):\n",
    "        print(f\"{eFUA_name} building data already downloaded\")\n",
    "    elif os.path.exists(f\"../data/buildings/buildings_{eFUA_name}_clean.gpkg\"):\n",
    "        print(f\"{eFUA_name} building data already downloaded and preprocessed\")\n",
    "    else:\n",
    "        print(f\"dowloading {eFUA_name} building data\")\n",
    "        \n",
    "        # get the geometries of all polygons\n",
    "        gdf = geopandas.read_parquet(f\"../data/{eFUA_ID}/polygons/\")\n",
    "        # make a square grid of cell width 10km on top of it\n",
    "        grid = create_grid_geometry(gdf, 10000)\n",
    "        # convert grid back to EPSG:4326 (for OSMnx download)\n",
    "        grid = grid.to_crs(\"EPSG:4326\")\n",
    "\n",
    "        # make temp subfolder for current city\n",
    "        os.makedirs(f\"../data/buildings/temp_{eFUA_name}/\", exist_ok=True)\n",
    "\n",
    "        # download building data for each grid cell separately\n",
    "        for ix, row in grid.iterrows():\n",
    "            # try/except (some grid cells don't contain any building data for which osmnx throws an error)\n",
    "            try: \n",
    "                buildings = osmnx.features_from_polygon(\n",
    "                    row.geometry,\n",
    "                    tags={\"building\": True},\n",
    "                    )\n",
    "            except:\n",
    "                buildings = geopandas.GeoDataFrame()\n",
    "            # if building data has been found for this grid cell:\n",
    "            if not buildings.empty:\n",
    "                # drop not needed columns\n",
    "                buildings = buildings.reset_index()\n",
    "                buildings = buildings[[\"osmid\", \"geometry\"]]\n",
    "                # drop points and linestrings, assert we only have polygons in the gdf\n",
    "                buildings = buildings.drop(\n",
    "                    buildings[buildings.geometry.type == \"Point\"].index, axis=0\n",
    "                ).reset_index(drop=True)\n",
    "                buildings = buildings.drop(\n",
    "                    buildings[buildings.geometry.type == \"LineString\"].index, axis=0\n",
    "                ).reset_index(drop=True)\n",
    "                # check that we now only have (multi)polygons in the data set\n",
    "                assert all(buildings.geometry.type.isin([\"Polygon\", \"MultiPolygon\"]))\n",
    "                # save \"cleaned\" data set for this grid cell\n",
    "                buildings.to_file(f\"../data/buildings/temp_{eFUA_name}/buildings_{ix}.gpkg\")        \n",
    "            del(buildings)\n",
    "\n",
    "        print(f\"all buildings for {eFUA_name} downloaded\")\n",
    "        del(grid, gdf, eFUA_name, eFUA_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine grid cell data sets for each city into one (already cleaned) data set\n",
    "\n",
    "for ix in sample_big.index:  \n",
    "    # get meta data for this city\n",
    "    eFUA_ID = int(sample_big.loc[ix, \"eFUA_ID\"])\n",
    "    eFUA_name = str(sample_big.loc[ix, \"eFUA_name\"])\n",
    "    if not os.path.exists(f\"../data/buildings/buildings_{eFUA_name}_clean.gpkg\"):\n",
    "        # initialize an empty gdf\n",
    "        gdf = geopandas.GeoDataFrame()\n",
    "        # combine data from all cells into one gdf\n",
    "        for filename in os.listdir(f\"../data/buildings/temp_{eFUA_name}/\"):\n",
    "            gdf_temp = geopandas.read_file(f\"../data/buildings/temp_{eFUA_name}/\" + filename)\n",
    "            gdf = pandas.concat([gdf, gdf_temp])\n",
    "        print(eFUA_name, len(gdf), \" before dedup\")\n",
    "        # drop duplicates\n",
    "        gdf.drop_duplicates(subset = \"osmid\", keep = \"first\", inplace = True, ignore_index = True)\n",
    "        print(eFUA_name, len(gdf), \" after dedup\")\n",
    "        # drop osmid column (not needed anymore)\n",
    "        gdf = gdf[[\"geometry\"]]\n",
    "        # explode multipolygons\n",
    "        gdf = gdf.explode(index_parts=False)\n",
    "        # save file\n",
    "        gdf.to_file(f\"../data/buildings/buildings_{eFUA_name}_clean.gpkg\", index = False)\n",
    "        print(f\"Cleaned data saved for {eFUA_name}\")\n",
    "    else:\n",
    "        print(f\"Cleaned data exists for {eFUA_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the temporary subfolders that contain building data per grid cell\n",
    "for eFUA_name in [\"London\", \"Dortmund\"]:\n",
    "    shutil.rmtree(f\"../data/buildings/temp_{eFUA_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual processing step in QGIS\n",
    "\n",
    "In this step, for each FUA, we visually checked in QGIS (by plotting and comparing all buildings, the OSM base layer, and the FUA shape) whether its building mapping in OSM is sufficiently complete as to be used for face artifact validation purposes. Next, in the list for each continent, we noted down cities with enough buildings mapped for validation.\n",
    "\n",
    "**Africa:** Abidjan, Conakry, Douala, Modagishu\n",
    "\n",
    "**Asia:** Aleppo, Kabul, Semarang, Seoul\n",
    "\n",
    "**South America:** Bucaramanga, Porto Alegre\n",
    "\n",
    "**North America:** Cincinnati, Dallas, Ottawa, Raleigh, Richmond, Rio Piedras [San Juan], Salt Lake City, San José, Washington D.C.\n",
    "\n",
    "**Europe:** Amsterdam, Belgrade, Chelyabinsk, Cologne, Dortmund, Helsinki, Katowice, Krakow, Liège, London, Nuremberg, Saratov, Vienna, Warsaw\n",
    "\n",
    "**Oceania:** Auckland\n",
    "\n",
    "**Discarded cities**, i.e. cities with enough buildings mapped for validation, but no artifact index threshold found: Comilla, Dhaka, Jombang, Monrovia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define FUA subsample for validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_for_validation = [\n",
    "    \"Abidjan\",\n",
    "    \"Conakry\",\n",
    "    \"Douala\",\n",
    "    \"Mogadishu\",  # Africa\n",
    "    \"Aleppo\",\n",
    "    \"Kabul\",\n",
    "    \"Semarang\",\n",
    "    \"Seoul\",  # Asia\n",
    "    \"Bucaramanga\",\n",
    "    \"Porto Alegre\",  # South America\n",
    "    \"Cincinnati\",\n",
    "    \"Dallas\",\n",
    "    \"Ottawa\",\n",
    "    \"Raleigh\",\n",
    "    \"Richmond\",\n",
    "    \"Río Piedras [San Juan]\",\n",
    "    \"Salt Lake City\",\n",
    "    \"San Jose\",\n",
    "    \"Washington D.C.\",  # North America\n",
    "    \"Auckland\",  # Oceania\n",
    "    \"Amsterdam\",\n",
    "    \"Belgrade\",\n",
    "    \"Chelyabinsk\",\n",
    "    \"Cologne\",\n",
    "    \"Dortmund\",\n",
    "    \"Helsinki\",\n",
    "    \"Katowice\",\n",
    "    \"Krakow\",\n",
    "    \"Liège\",\n",
    "    \"London\",\n",
    "    \"Nuremberg\",\n",
    "    \"Saratov\",\n",
    "    \"Vienna\",\n",
    "    \"Warsaw\",  # Europe\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing of building data for validation subsample**\n",
    "\n",
    "Clean the building data (will only be needed for the cities where we did download the building data in one go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_validation = sample[sample.eFUA_name.isin(cities_for_validation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter warnings about GeoParquet implementation.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*initial implementation of Parquet.*\")\n",
    "\n",
    "for ix, row in tqdm(sample_validation.iterrows(), total=len(sample_validation)):\n",
    "    city = row.eFUA_name\n",
    "    myid = row.eFUA_ID\n",
    "\n",
    "    if os.path.exists(f\"../data/buildings/buildings_{city}_clean.gpkg\"):\n",
    "        print(f\"{city} building data already preprocessed\")\n",
    "    elif not os.path.exists(f\"../data/buildings/buildings_{city}.gpkg\"):\n",
    "        print(f\"{city} building data not downloaded yet\")\n",
    "    else:\n",
    "        polygons = geopandas.read_parquet(f\"../data/{int(row.eFUA_ID)}/polygons/\")\n",
    "        buildings = geopandas.read_file(f\"../data/buildings/buildings_{city}.gpkg\")\n",
    "        buildings = buildings.to_crs(polygons.crs)\n",
    "\n",
    "        # drop tags (not needed for analysis)\n",
    "        buildings = buildings[[\"geometry\"]]\n",
    "\n",
    "        # drop points, assert we only have polygons in the gdf\n",
    "        buildings = buildings.drop(\n",
    "            buildings[buildings.geometry.type == \"Point\"].index, axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        buildings = buildings.drop(\n",
    "            buildings[buildings.geometry.type == \"LineString\"].index, axis=0\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # explode multipolygons\n",
    "        buildings = buildings.explode(index_parts=False)\n",
    "\n",
    "        # check that we now only have polygons in the data set\n",
    "        assert all(buildings.geometry.type == \"Polygon\")\n",
    "\n",
    "        # save \"cleaned\" data set\n",
    "        buildings.to_file(f\"../data/buildings/buildings_{city}_clean.gpkg\", index=0)\n",
    "\n",
    "        # delete original data set\n",
    "        os.remove(f\"../data/buildings/buildings_{city}.gpkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in peak results (face artifact thresholds)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = json.load(open(\"../results/04_peaks_results.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify urban blocks and calculate block/building overlap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/validation_temp\", exist_ok=True)\n",
    "\n",
    "# Filter warnings about GeoParquet implementation.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*initial implementation of Parquet.*\")\n",
    "\n",
    "for ix, row in tqdm(sample_validation.iterrows(), total=len(sample_validation)):\n",
    "    city = row.eFUA_name\n",
    "    myid = int(row.eFUA_ID)\n",
    "    # read in buildings and polygons\n",
    "    buildings = geopandas.read_file(f\"../data/buildings/buildings_{city}_clean.gpkg\")\n",
    "    polygons = geopandas.read_parquet(f\"../data/{myid}/polygons/\")\n",
    "    buildings = buildings.to_crs(polygons.crs)\n",
    "    assert buildings.crs == polygons.crs\n",
    "\n",
    "    # find predicted artifacts\n",
    "    option = \"circular_compactness_index\"\n",
    "    threshold = results[city][option][\"threshold\"]\n",
    "    polygons[\"is_artifact\"] = False # set default to False\n",
    "    polygons.loc[\n",
    "        polygons[\"circular_compactness_index\"] <= threshold, \"is_artifact\"\n",
    "    ] = True # set to True for polygons where index is below the threshold\n",
    "\n",
    "    # find overlap of polygons with buildings\n",
    "    mytree = strtree.STRtree(geoms=buildings.geometry)\n",
    "    q = mytree.query(polygons.geometry, predicate=\"intersects\")\n",
    "    # q[0] ...polygon indeces\n",
    "    # q[1] ...building indeces\n",
    "\n",
    "    # find out whether polygons contain buldings or not\n",
    "    polygons[\"intersects_buildings\"] = False  # set default to False\n",
    "    polygons.loc[numpy.unique(q[0]), \"intersects_buildings\"] = True\n",
    "    # if index is in query, change to True\n",
    "\n",
    "    #### find absolute value of built area in polygon [in square meters]\n",
    "\n",
    "    # column of total area of polygon\n",
    "    polygons[\"area\"] = polygons.area\n",
    "\n",
    "    # column of building area within polygon (default to 0)\n",
    "    polygons[\"intersection_area\"] = 0\n",
    "\n",
    "    # replace 0 with area of unary union of buildings intersecting that polygon\n",
    "    for polygon_index in numpy.unique(q[0]):\n",
    "        # for this polygon, get indeces of buildings intersecting it\n",
    "        building_indeces = q[1][numpy.where(q[0] == polygon_index)]\n",
    "\n",
    "        # get area of intersection\n",
    "        intersection_area = (\n",
    "            polygons.loc[polygon_index, \"geometry\"]\n",
    "            .intersection(buildings.loc[building_indeces, \"geometry\"].unary_union)\n",
    "            .area\n",
    "        )\n",
    "\n",
    "        # add to polygon table\n",
    "        polygons.loc[polygon_index, \"intersection_area\"] = intersection_area\n",
    "\n",
    "    # drop not needed columns that were created in the process\n",
    "    polygons = polygons.drop(columns = [\"area\", \"intersects_buildings\"])\n",
    "\n",
    "    # save polygons to a partitioned GeoParquet, overwriting original polygons\n",
    "    polygons = dask_geopandas.from_geopandas(polygons, npartitions=10)\n",
    "    polygons.to_parquet(f\"../data/{int(myid)}/polygons/\")\n",
    "\n",
    "    del(buildings, polygons, mytree, q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "35f35e1f0bb92e7ac05765c87ada263d1a2173e12d4a4460e46e5d1567b982a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
